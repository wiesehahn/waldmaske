{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Earth_Engine_TensorFlow_AI_Platform.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wiesehahn/waldmaske/blob/master/notebooks/ee_tensorflow_mask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSIfBsgi8dNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Copyright 2020 Wiesehahn { display-mode: \"form\" }\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1xZ1CPi3Nw",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
        "<a target=\"_blank\"  href=\"https://colab.research.google.com/gist/wiesehahn/8ec5e2065c01873e84c896508a98b3dd/earth_engine_tensorflow_ai_platform.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://gist.github.com/wiesehahn/8ec5e2065c01873e84c896508a98b3dd#file-earth_engine_tensorflow_ai_platform-ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub Gist</a></td></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC8adBmw-5m3",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is an Earth Engine <> TensorFlow notebook. It is heavily inspired and adapted from [this](https://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/Earth_Engine_TensorFlow_AI_Platform.ipynb) and [this](http://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/TF_demo1_keras.ipynb) demonstration notebooks. It demonstrates a per-pixel neural network implemented in a way that allows the trained model to be hosted on [Google AI Platform](https://cloud.google.com/ai-platform) and used in Earth Engine for interactive prediction from an `ee.Model.fromAIPlatformPredictor`.\n",
        "\n",
        "**Running this demo may incur charges to your Google Cloud Account!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiTyR3FNlv-O",
        "colab_type": "text"
      },
      "source": [
        "# Setup software libraries\n",
        "\n",
        "Import software libraries and/or authenticate as necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HsyDopq-yy2b"
      },
      "source": [
        "## Authenticate to Colab and Cloud\n",
        "\n",
        "Google Cloud Storage bucket will serve as a bridge between GEE and Colab. To read/write from a Google Cloud Storage bucket to which you have access, it's necessary to authenticate (as yourself).  *This should be the same account you use to login to Earth Engine*.  When you run the code below, it will display a link in the output to an authentication page in your browser.  Follow the link to a page that will let you grant permission to the Cloud SDK to access your resources.  Copy the code from the permissions page back into this notebook and press return to complete the process.\n",
        "\n",
        "(You may need to run this again if you get a credentials error later.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYyTIPLsvMWl",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejxa1MQjEGv9",
        "colab_type": "text"
      },
      "source": [
        "## Authenticate to Earth Engine\n",
        "\n",
        "Authenticate to Earth Engine the same way you did to the Colab notebook.  Specifically, run the code to display a link to a permissions page.  This gives you access to your Earth Engine account.  *This should be the same account you used to login to Cloud previously*.  Copy the code from the Earth Engine permissions page back into the notebook and press return to complete the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzwiVqbcmJIX",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEJjst1XpJDn",
        "colab_type": "text"
      },
      "source": [
        "## Initialize Geemap \n",
        "\n",
        "Import [Geemap package](https://github.com/giswqs/geemap)\n",
        "\n",
        "(A Python package for interactive mapping with Google Earth Engine, ipyleaflet, and ipywidgets.)\n",
        "\n",
        "Note that Google Colab currently does not support ipyleaflet. Therefore, you should use import geemap.eefolium instead of import geemap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u-JiPAWmA0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install geemap\n",
        "import geemap.eefolium as emap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xhZiXmnSyy2l"
      },
      "source": [
        "## Test the TensorFlow installation\n",
        "\n",
        "Import TensorFlow and check the version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "WjOh_CJeyy2m",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w6hPSVmYyy2p"
      },
      "source": [
        "## Test the Folium installation\n",
        "\n",
        "We might use the Folium library for visualization where Geemap does not work.  Import the library and check the version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "frodQp2syy2q",
        "colab": {}
      },
      "source": [
        "import folium\n",
        "print(folium.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrXLkJC2QJdP",
        "colab_type": "text"
      },
      "source": [
        "# Define variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dbhC2rzV642R",
        "colab": {}
      },
      "source": [
        "# Your Earth Engine username. This is used to import a classified image\n",
        "# into your Earth Engine assets folder.\n",
        "USER_NAME = 'wiesehahn'\n",
        "\n",
        "# Replace with your Google Cloud Project\n",
        "PROJECT = 'forest-201911'\n",
        "\n",
        "# Cloud Storage bucket into which training, testing and prediction \n",
        "# datasets will be written. You must be able to write into this bucket.\n",
        "OUTPUT_BUCKET = 'gee_forest-mask'\n",
        "\n",
        "# File names for the training and testing datasets. These TFRecord files\n",
        "# will be exported from Earth Engine into the Cloud Storage bucket.\n",
        "TRAIN_FILE_PREFIX = 'Training_lucas'\n",
        "TEST_FILE_PREFIX = 'Testing_lucas'\n",
        "file_extension = '.tfrecord.gz'\n",
        "TRAIN_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TRAIN_FILE_PREFIX + file_extension\n",
        "TEST_FILE_PATH = 'gs://' + OUTPUT_BUCKET + '/' + TEST_FILE_PREFIX + file_extension\n",
        "\n",
        "# The name and path of the Earth Engine asset to be created \n",
        "OUTPUT_ASSET_ID = 'users/' + USER_NAME + '/waldmaske/classification/map/tf_prob_lucas_v0'\n",
        "\n",
        "\n",
        "# Use these bands for prediction. B* represent S2 reflectance values for corresponding \n",
        "# bands in spring, and B*_1 represent reflectance values in summer.\n",
        "BANDS = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', \n",
        "         'B2_1', 'B3_1', 'B4_1', 'B5_1', 'B6_1', 'B7_1', 'B8_1', 'B8A_1', 'B9_1', 'B11_1', 'B12_1']\n",
        "\n",
        "# This is a training/testing dataset of points with known land cover labels.\n",
        "LABEL_DATA = 'users/wiesehahn/waldmaske/classification/reference/lucas_filtered'\n",
        "# The labels, consecutive integer indices starting from zero, are stored in this property, set on each point.\n",
        "LABEL = 'class'\n",
        "# Number of label values, i.e. number of classes in the classification (here 'forest'/'no-forest')\n",
        "N_CLASSES = 2\n",
        "\n",
        "# These names are used to specify properties in the export of\n",
        "# training/testing data and to define the mapping between names and data\n",
        "# when reading into TensorFlow datasets.\n",
        "FEATURE_NAMES = list(BANDS)\n",
        "FEATURE_NAMES.append(LABEL)\n",
        "\n",
        "# List of fixed-length features, all of which are float32.\n",
        "columns = [tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in FEATURE_NAMES]\n",
        "\n",
        "# Dictionary with feature names as keys, fixed-length features as values.\n",
        "FEATURES_DICT = dict(zip(FEATURE_NAMES, columns))\n",
        "\n",
        "\n",
        "# define region of interest\n",
        "# General ROI is Germany while test area for prediction is area around Göttingen\n",
        "GAUL = ee.FeatureCollection(\"FAO/GAUL_SIMPLIFIED_500m/2015/level1\")  \n",
        "DE = GAUL.filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n",
        "ROI = DE\n",
        "\n",
        "TESTAREA = ee.Geometry.Polygon(\n",
        "        [[[9.940686323657957, 51.5633863622234],\n",
        "          [9.940686323657957, 51.504442609479455],\n",
        "          [10.057416059986082, 51.504442609479455],\n",
        "          [10.057416059986082, 51.5633863622234]]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcjQnHH8zT4q",
        "colab_type": "text"
      },
      "source": [
        "# Create Training and Testing data\n",
        "\n",
        "Public available data from the 'Land Use and Coverage Area frame Survey' ([LUCAS](https://ec.europa.eu/eurostat/web/lucas/overview)) for 2018 was used as reference data. In order to use it for this project within Earth Engine the data had to be filtered and uploaded to EE as an asset. To download and filter the data similar to [Weigand et. al, 2020](https://www.sciencedirect.com/science/article/pii/S0303243419307317) a Script was applied in R ([see here](https://gist.github.com/wiesehahn/27bd929f54176bfacd246aabc970bf3c#file-lucas_filtered-r)).\n",
        "\n",
        "With this Reference data a model is built based on Sentinel-2 predictor variables (`BANDS`) from two time periods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHe1Wgu-9fn5",
        "colab_type": "text"
      },
      "source": [
        "## Load reference data\n",
        "\n",
        "Load the labeled points from existing Earth Engine asset.  Each point in this table has a property called `class` that stores the label, encoded as an integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-UH2Shy9zd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref_raw = ee.Collection.loadTable(LABEL_DATA)\n",
        "ref_raw = ref_raw.filterBounds(ROI)\n",
        "label = LABEL\n",
        "\n",
        "print('number of reference points:', ref_raw.size().getInfo());"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7UIg3NuyON6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Map = emap.Map()\n",
        "Map.centerObject(ROI, 8)\n",
        "\n",
        "\n",
        "# Adds Earth Engine layers to Map\n",
        "Map.addLayer(TESTAREA,{'color': 'FF0000'},'test area', True, 0.3)\n",
        "Map.addLayer(ref_raw, {'color': 'FFFFFF'}, 'reference', True, 0.5)\n",
        "\n",
        "# Display the Map\n",
        "Map.addLayerControl()\n",
        "Map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EJfjgelSOpN",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Sentinel-2 imagery\n",
        "\n",
        "Create a cloud-masked composite of Sentinel-2 surface reflectance imagery from 2019 and 2020. Actually, two composites are generated for spring (doy 75-150) and summer (doy 150-270) which are then combined. In a pre-study the predictive power of different seasons was examined for a random forest classifier [see here](https://code.earthengine.google.com/569e88cfdd90edb1b66930f61b8867c2?noload=true). The results suggested that spring and summer are more suitable than winter and autumn. Although this might be different in another model we kept these variables as it is hard to get entirely cloud-free composites for autumn and winter seasons.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zdKcTpWDAob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to mask clouds using SCL and MSK_CLDPRB bands\n",
        "def maskClouds_opt2(image):\n",
        "  # ESA Cloud Probability Map\n",
        "  cloudProb = image.select('MSK_CLDPRB')\n",
        "  # ESA Scene Classification\n",
        "  scl = image.select('SCL')\n",
        "  shadow = scl.eq(3)\n",
        "  cirrus = scl.eq(10)\n",
        "  mask = cloudProb.lt(5).And((cirrus).neq(1)).And((shadow).neq(1))\n",
        "  return image.updateMask(mask)\n",
        "\n",
        "# Load Sentinel-2 surface reflectance data \n",
        "s2 = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "      .filter(ee.Filter.calendarRange(2019,2020, 'year'))\n",
        "      .filterBounds(ROI)\n",
        "      .map(maskClouds_opt2)\n",
        "      .select('B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12'))\n",
        "\n",
        "# reduce images by season  \n",
        "spring = (s2\n",
        "          .filter(ee.Filter.calendarRange(75, 150, 'day_of_year'))\n",
        "          .reduce(ee.Reducer.percentile([33]))\n",
        "          .rename('B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12'))\n",
        "\n",
        "summer = (s2\n",
        "          .filter(ee.Filter.calendarRange(150, 270, 'day_of_year'))\n",
        "          .reduce(ee.Reducer.percentile([33]))\n",
        "          .rename('B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12'))\n",
        "\n",
        "# merge in one image\n",
        "composite = spring.addBands(summer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJrku7FD9IUQ",
        "colab_type": "text"
      },
      "source": [
        "Display predictor images as RGB (this might take a long tim to compute!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIFAUH-d6kFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Map = emap.Map()\n",
        "# Map.centerObject(TESTAREA, 13)\n",
        "\n",
        "# visParams ={'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 1500}\n",
        "\n",
        "# # Adds Earth Engine layers to Map\n",
        "# Map.addLayer(spring, visParams,'spring', True, 1)\n",
        "# Map.addLayer(summer, visParams, 'summer', True, 1)\n",
        "\n",
        "# # Display the Map\n",
        "# Map.addLayerControl()\n",
        "# Map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEeyPf3zSPct",
        "colab_type": "text"
      },
      "source": [
        "## Add pixel values of the composite to labeled points\n",
        "\n",
        "Here we overlay the points on imagery to get predictor variables along with labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOedOKyRExHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = composite.sampleRegions( \n",
        "   collection= ref_raw,\n",
        "   properties= [LABEL],\n",
        "   scale= 10,\n",
        "   tileScale= 16,\n",
        "   geometries= True\n",
        ").randomColumn('random')\n",
        "\n",
        "# remap values in two classes (forest/ no-forest)\n",
        "reference = sample.remap(\n",
        "  # [0:T3, 1:A, 2:S, 3:T1, 4:T2, 5:V1, 6:V2, 7:W]\n",
        "  lookupIn=  [0,1,2,3,4,5,6,7], \n",
        "  # [0:Forest, 1:No-Forest]\n",
        "  lookupOut= [0,1,1,0,0,1,1,1],\n",
        "  columnName= 'class')\n",
        "\n",
        "split = 0.7 # 70% training, 30% validation.\n",
        "training = reference.filter(ee.Filter.lt('random', split))\n",
        "testing = reference.filter(ee.Filter.gte('random', split))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNc7a2nRR4MI",
        "colab_type": "text"
      },
      "source": [
        "## Export the training and testing data\n",
        "\n",
        "Now that there's training and testing data , it's time to materialize the datasets in a place where the TensorFlow model has access to them (Unfortunately, you cannot use Tensorflow directly in Earth Engine). You can do that by exporting the training and testing datasets to tables in TFRecord format ([learn more about TFRecord format](https://www.tensorflow.org/tutorials/load_data/tf-records)) in your Cloud Storage bucket, since both GEE and GCS and Tensorflow can access to them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb-aPvQc0Xvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure you can see the output bucket. You must have write access.\n",
        "print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + OUTPUT_BUCKET) \n",
        "    else 'Can not find output Cloud Storage bucket.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtoqj0Db1TmJ",
        "colab_type": "text"
      },
      "source": [
        "Once you've verified the existence of the intended output bucket, run the exports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVNQzg8R6Wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the tasks.\n",
        "training_task = ee.batch.Export.table.toCloudStorage(\n",
        "  collection=training,\n",
        "  description='Training Export',\n",
        "  fileNamePrefix=TRAIN_FILE_PREFIX,\n",
        "  bucket=OUTPUT_BUCKET,\n",
        "  fileFormat='TFRecord',\n",
        "  selectors=FEATURE_NAMES)\n",
        "\n",
        "testing_task = ee.batch.Export.table.toCloudStorage(\n",
        "  collection=testing,\n",
        "  description='Testing Export',\n",
        "  fileNamePrefix=TEST_FILE_PREFIX,\n",
        "  bucket=OUTPUT_BUCKET,\n",
        "  fileFormat='TFRecord',\n",
        "  selectors=FEATURE_NAMES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF4WGIekaS2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start the tasks.\n",
        "if tf.io.gfile.exists(TRAIN_FILE_PATH):\n",
        "  print('Train file already exists')\n",
        "else:\n",
        "  training_task.start()\n",
        "\n",
        "if tf.io.gfile.exists(TEST_FILE_PATH):\n",
        "  print('Test file already exists')\n",
        "else:\n",
        "  testing_task.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7nFLuySISeC",
        "colab_type": "text"
      },
      "source": [
        "Monitor task progress \n",
        "\n",
        "(Make sure the training and testing tasks are completed before continuing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEWvS5ekcEq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print all tasks.\n",
        "pprint(ee.batch.Task.list())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5c5wdnKXLbIa"
      },
      "source": [
        "Check existence of the exported files \n",
        "\n",
        "(If you've seen the status of the export tasks change to `COMPLETED`, then check for the existince of the files in the output Cloud Storage bucket)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sosRFEDdOMA",
        "colab_type": "text"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43-c0JNFI_m6",
        "colab_type": "text"
      },
      "source": [
        "### Check existence of the data files\n",
        "\n",
        "Check that you have permission to read the files in the output Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDZfNl6yc0Kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Found training file.' if tf.io.gfile.exists(TRAIN_FILE_PATH) \n",
        "    else 'No training file found.')\n",
        "print('Found testing file.' if tf.io.gfile.exists(TEST_FILE_PATH) \n",
        "    else 'No testing file found.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4jGTrEfz-1",
        "colab_type": "text"
      },
      "source": [
        "## Read into a `tf.data.Dataset`\n",
        "\n",
        "Here we are going to read a file in Cloud Storage into a `tf.data.Dataset`.  ([these TensorFlow docs](https://www.tensorflow.org/guide/data) explain more about reading data into a `tf.data.Dataset`).  Check that you can read examples from the file.  The purpose here is to ensure that we can read from the file without an error.  The actual content is not necessarily human readable.  Note that we will use all data for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3PKyDQW8Vpx",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataset from the TFRecord file in Cloud Storage.\n",
        "train_dataset = tf.data.TFRecordDataset([TRAIN_FILE_PATH],\n",
        "                                        compression_type='GZIP')\n",
        "\n",
        "# Print the first record to check.\n",
        "print(iter(train_dataset).next())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNfaUPbcjuCO",
        "colab_type": "text"
      },
      "source": [
        "## Parse the dataset\n",
        "\n",
        "Now we need to make a parsing function for the data in the TFRecord files.  The data comes in flattened 2D arrays per record and we want to use the first part of the array for input to the model and the last element of the array as the class label.  The parsing function reads data from a serialized `Example` proto (i.e. [`example.proto`](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/core/example/example.proto)) into a dictionary in which the keys are the feature names and the values are the tensors storing the value of the features for that example.  ([Learn more about parsing `Example` protocol buffer messages](https://www.tensorflow.org/programmers_guide/datasets#parsing_tfexample_protocol_buffer_messages))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Q0g3fBj2kD",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of the predictors dictionary and the LABEL, cast to an `int32`.\n",
        "  \"\"\"\n",
        "  parsed_features = tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "  labels = parsed_features.pop(LABEL)\n",
        "  return parsed_features, tf.cast(labels, tf.int32)\n",
        "\n",
        "# Map the function over the dataset.\n",
        "parsed_dataset = train_dataset.map(parse_tfrecord, num_parallel_calls=4)\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "# Print the first parsed record to check.\n",
        "pprint(iter(parsed_dataset).next())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb8EyNT4Xnhb",
        "colab_type": "text"
      },
      "source": [
        "Note that each record of the parsed dataset contains a tuple.  The first element of the tuple is a dictionary with bands names for keys and tensors storing the pixel data for values.  The second element of the tuple is tensor storing the class label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLCsxWOuEBmE",
        "colab_type": "text"
      },
      "source": [
        "## Create additional features\n",
        "\n",
        "Another thing we might want to do as part of the input process is to create new features, for example NDVI, a vegetation index computed from reflectance in two spectral bands.  Here are some helper functions for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT6v2RM_EB1E",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def normalized_difference(a, b):\n",
        "  \"\"\"Compute normalized difference of two inputs.\n",
        "\n",
        "  Compute (a - b) / (a + b).  If the denomenator is zero, add a small delta.\n",
        "\n",
        "  Args:\n",
        "    a: an input tensor with shape=[1]\n",
        "    b: an input tensor with shape=[1]\n",
        "\n",
        "  Returns:\n",
        "    The normalized difference as a tensor.\n",
        "  \"\"\"\n",
        "  nd = (a - b) / (a + b)\n",
        "  nd_inf = (a - b) / (a + b + 0.000001)\n",
        "  return tf.where(tf.math.is_finite(nd), nd, nd_inf)\n",
        "\n",
        "def add_NDVI(features, label):\n",
        "  \"\"\"Add NDVI to the dataset.\n",
        "  Args:\n",
        "    features: a dictionary of input tensors keyed by feature name.\n",
        "    label: the target label\n",
        "\n",
        "  Returns:\n",
        "    A tuple of the input dictionary with an NDVI tensor added and the label.\n",
        "  \"\"\"\n",
        "  features['NDVI'] = normalized_difference(features['B5'], features['B4'])\n",
        "  features['NDVI_1'] = normalized_difference(features['B5_1'], features['B4_1'])\n",
        "  return features, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOOAsgZslhx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add NDVI.\n",
        "# parsed_dataset = parsed_dataset.map(add_NDVI)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZwSBGX27Bfy",
        "colab_type": "text"
      },
      "source": [
        "## Adjust dimension and shape\n",
        "\n",
        "Turn the dictionary of *{name: tensor,...}* into a 1x1xP array of values, where P is the number of predictors.  Turn the label into a 1x1x`N_CLASSES` array of indicators (i.e. one-hot vector), in order to use a categorical crossentropy-loss function.  Return a tuple of (predictors, indicators where each is a three dimensional array; the first two dimensions are spatial x, y (i.e. 1x1 kernel)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABZvVGZw7BsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inputs as a tuple.  Make predictors 1x1xP and labels 1x1xN_CLASSES.\n",
        "def to_tuple(inputs, label):\n",
        "  return (tf.expand_dims(tf.transpose(list(inputs.values())), 1),\n",
        "          tf.expand_dims(tf.one_hot(indices=label, depth=N_CLASSES), 1))\n",
        "\n",
        "input_dataset = parsed_dataset.map(to_tuple)\n",
        "# Check the first one.\n",
        "pprint(iter(input_dataset).next())\n",
        "\n",
        "input_dataset = input_dataset.shuffle(128).batch(8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEx1RAXOZQkS",
        "colab_type": "text"
      },
      "source": [
        "# Model setup\n",
        "\n",
        "Make a densely-connected convolutional model, where the convolution occurs in a 1x1 kernel.  This is exactly analagous to the model generated in [this example notebook](http://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/TF_demo1_keras.ipynb), but operates in a convolutional manner in a 1x1 kernel.  This allows Earth Engine to apply the model spatially, as demonstrated below.\n",
        "\n",
        "Note that the model used here is purely for demonstration purposes and hasn't gone through any performance tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9pWa54oG-xl",
        "colab_type": "text"
      },
      "source": [
        "## Create the Keras model\n",
        "\n",
        "Before we create the model, there's still a wee bit of pre-processing to get the data into the right input shape and a format that can be used with cross-entropy loss.  Specifically, Keras expects a list of inputs and a one-hot vector for the class. (See [the Keras loss function docs](https://keras.io/losses/), [the TensorFlow categorical identity docs](https://www.tensorflow.org/guide/feature_columns#categorical_identity_column) and [the `tf.one_hot` docs](https://www.tensorflow.org/api_docs/python/tf/one_hot) for details).\n",
        "\n",
        "Here we will use a simple neural network model with a 64 node hidden layer.  Once the dataset has been prepared, define the model, compile it, fit it to the training data.  See [the Keras `Sequential` model guide](https://keras.io/getting-started/sequential-model-guide/) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCZq3VNpG--G",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Define the layers in the model.  Note the 1x1 kernels.\n",
        "model = tf.keras.models.Sequential([\n",
        "  #change to number of input bands\n",
        "  tf.keras.layers.Input((None, None, 22,)), \n",
        "  tf.keras.layers.Conv2D(64, (1,1), activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Conv2D(N_CLASSES, (1,1), activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "# Compile the model with the specified loss and optimizer functions.\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model to the training data. \n",
        "model.fit(x=input_dataset, epochs=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shbr6cSXShRg",
        "colab_type": "text"
      },
      "source": [
        "## Save the trained model\n",
        "\n",
        "Export the trained model to TensorFlow `SavedModel` format in your cloud storage bucket.  The [Cloud Platform storage browser](https://console.cloud.google.com/storage/browser) is useful for checking on these saved models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgg7MTXfS1PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_DIR = 'gs://' + OUTPUT_BUCKET + '/lucas_pixel_model'\n",
        "model.save(MODEL_DIR, save_format='tf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keijPyVQTIAq",
        "colab_type": "text"
      },
      "source": [
        "# EEification\n",
        "\n",
        "EEIfication prepares the model for hosting on [Google AI Platform](https://cloud.google.com/ai-platform).  Learn more about EEification from [this doc](https://developers.google.com/earth-engine/tensorflow#interacting-with-models-hosted-on-ai-platform).  First, get (and SET) input and output names of the nodes.  **CHANGE THE OUTPUT NAME TO SOMETHING THAT MAKES SENSE FOR YOUR MODEL!**  Keep the input name of 'array', which is how you'll pass data into the model (as an array image)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w49O7n5oTS4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.tools import saved_model_utils\n",
        "\n",
        "meta_graph_def = saved_model_utils.get_meta_graph_def(MODEL_DIR, 'serve')\n",
        "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
        "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
        "\n",
        "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
        "# model only has a single input and a single output.\n",
        "input_name = None\n",
        "for k,v in inputs.items():\n",
        "  input_name = v.name\n",
        "  break\n",
        "\n",
        "output_name = None\n",
        "for k,v in outputs.items():\n",
        "  output_name = v.name\n",
        "  break\n",
        "\n",
        "# Make a dictionary that maps Earth Engine outputs and inputs to\n",
        "# AI Platform inputs and outputs, respectively.\n",
        "import json\n",
        "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
        "output_dict = \"'\" + json.dumps({output_name: \"lucas\"}) + \"'\"\n",
        "print(input_dict)\n",
        "print(output_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX2icXa1UdFF",
        "colab_type": "text"
      },
      "source": [
        "## Run the EEifier\n",
        "\n",
        "The actual EEification is handled by the `earthengine model prepare` command.  Note that you will need to set your Cloud Project prior to running the command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYmH_wCOUhIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put the EEified model next to the trained model directory.\n",
        "EEIFIED_DIR = 'gs://' + OUTPUT_BUCKET + '/eeified_pixel_model'\n",
        "\n",
        "if tf.io.gfile.exists(EEIFIED_DIR):\n",
        "  print('EEified model exists already')\n",
        "else:\n",
        "  # You need to set the project before using the model prepare command.\n",
        "  !earthengine set_project {PROJECT}\n",
        "  !earthengine model prepare --source_dir {MODEL_DIR} --dest_dir {EEIFIED_DIR} --input {input_dict} --output {output_dict}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_uTqQAaVTIK",
        "colab_type": "text"
      },
      "source": [
        "# Deploy and host the EEified model on AI Platform\n",
        "\n",
        "Now there is another TensorFlow `SavedModel` stored in `EEIFIED_DIR` ready for hosting by AI Platform.  Do that from the `gcloud` command line tool, installed in the Colab runtime by default.  Note that the `MODEL_NAME` must be unique.  If you already have a model by that name, either name a new model or a new version of the old model.  The [Cloud Console AI Platform models page](https://console.cloud.google.com/ai-platform/models) is useful for monitoring your models.\n",
        "\n",
        "**If you change anything about the trained model, you'll need to re-EEify it and create a new version!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RZRRzcfVu5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = 'pixel_lucas_model'\n",
        "VERSION_NAME = 'v02'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpx0iielBSAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gcloud ai-platform models create {MODEL_NAME} --project {PROJECT}\n",
        "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
        "  --project {PROJECT} \\\n",
        "  --model {MODEL_NAME} \\\n",
        "  --origin {EEIFIED_DIR} \\\n",
        "  --framework \"TENSORFLOW\" \\\n",
        "  --runtime-version=2.1 \\\n",
        "  --python-version=3.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aTGza-rWIjp",
        "colab_type": "text"
      },
      "source": [
        "# Connect to the hosted model from Earth Engine\n",
        "1. load input image\n",
        "2. Connect to the hosted model.\n",
        "3. Use the model to make predictions.\n",
        "4. Display the results.\n",
        "\n",
        "Note that it takes the model a couple minutes to spin up and make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2OsyrJ7HAhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Turn into an array image for input to the model.\n",
        "array_image = composite.float().toArray()\n",
        "\n",
        "# Point to the model hosted on AI Platform.\n",
        "model = ee.Model.fromAiPlatformPredictor(\n",
        "    projectName=PROJECT,\n",
        "    modelName=MODEL_NAME,\n",
        "    version=VERSION_NAME,\n",
        "    # Can be anything, but don't make it too big.\n",
        "    inputTileSize=[8, 8],\n",
        "    # # Keep this the same as your training data.\n",
        "    # proj=ee.Projection('EPSG:4326').atScale(10),\n",
        "    # fixInputProj=True,\n",
        "    # Note the names here need to match what you specified in the\n",
        "    # output dictionary you passed to the EEifier.\n",
        "    outputBands={'lucas': {\n",
        "        'type': ee.PixelType.float(),\n",
        "        'dimensions': 1\n",
        "      }\n",
        "    },\n",
        ")\n",
        "\n",
        "# model.predictImage outputs a one dimensional array image that\n",
        "# packs the output nodes of your model into an array.  These\n",
        "# are class probabilities that you need to unpack into a \n",
        "# multiband image with arrayFlatten().  If you want class\n",
        "# labels, use arrayArgmax() as follows.\n",
        "predictions = model.predictImage(array_image)\n",
        "probabilities = predictions.arrayFlatten([['forest', 'no-forest']])\n",
        "label = predictions.arrayArgmax().arrayGet([0]).rename('label')\n",
        "\n",
        "#create an export task\n",
        "export_task = ee.batch.Export.image.toAsset(\n",
        "    image= probabilities,\n",
        "    description= 'Probability Export', \n",
        "    assetId= OUTPUT_ASSET_ID,\n",
        "    region = TESTAREA, \n",
        "    scale = 10)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pANkF4mKzab2",
        "colab": {}
      },
      "source": [
        "# Start the export task\n",
        "export_task.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7y37A11T039m"
      },
      "source": [
        "### Monitor task progress\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKZeZswloP11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print all tasks.\n",
        "pprint(ee.batch.Task.list())\n",
        "\n",
        "import time\n",
        "\n",
        "while export_task.active():\n",
        "  print('Polling for task (id: {}).'.format(export_task.id))\n",
        "  time.sleep(60)\n",
        "print('Done with image export.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52E8ZuMq1cus",
        "colab_type": "text"
      },
      "source": [
        "## Display Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GrJu_SFfg_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load classified image and get info\n",
        "predictions_image = ee.Image(OUTPUT_ASSET_ID)\n",
        "print(predictions_image.bandNames().getInfo())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiTYkQBO1gyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Map = emap.Map()\n",
        "Map.centerObject(TESTAREA, 13)\n",
        "\n",
        "# Sets visualization parameters\n",
        "probability_vis = {'bands': ['no-forest','forest'], 'max': 0.5}\n",
        "visParams ={'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 1500}\n",
        "\n",
        "# Adds Earth Engine layers to Map\n",
        "Map.addLayer(TESTAREA,{},'test area', True, 0.3)\n",
        "Map.addLayer(predictions_image, probability_vis, 'Forest probability', True, 0.7)\n",
        "Map.addLayer(composite, visParams, 'Spring RGB (input image)', True, 0.7)\n",
        "\n",
        "\n",
        "# Display the Map\n",
        "Map.addLayerControl()\n",
        "Map"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}